{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A deeper dive into loading data"
      ],
      "metadata": {
        "id": "X9ZJMm3RILYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "animals = pd.read_csv('animals.csv')\n",
        "\n",
        "\n",
        "# Define input features\n",
        "features = animals.iloc[:, 1:-1]\n",
        "X = features.to_numpy()\n",
        "print(X)\n",
        "\n",
        " # Define target features (ground truth)\n",
        "target = animals.iloc[:, -1]\n",
        "y = target.to_numpy()"
      ],
      "metadata": {
        "id": "Tq7NLjqMLmWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Instantiate dataset class\n",
        "dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float())\n",
        "\n",
        "# Access an individual sample\n",
        "sample = dataset[0]\n",
        "input_sample, label_sample = sample\n",
        "print('input sample:', input_sample)\n",
        "print('label_sample:', label_sample)"
      ],
      "metadata": {
        "id": "UtlcEh2iL63W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 2\n",
        "shuffle = True\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        " # Iterate over the dataloader\n",
        "for batch_inputs, batch_labels in dataloader:\n",
        "    print('batch inputs', batch_inputs)\n",
        "    print('batch labels', batch_labels)"
      ],
      "metadata": {
        "id": "dW3SBDL4ME_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create a TensorDataset using the torch_features and the torch_target tensors provided (in this order).\n",
        "Return the last element of the dataset.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "np_features = np.array(np.random.rand(12, 8))\n",
        "np_target = np.array(np.random.rand(12, 1))\n",
        "\n",
        "torch_features = torch.tensor(np_features)\n",
        "torch_target = torch.tensor(np_target)\n",
        "\n",
        "# Create a TensorDataset from two tensors\n",
        "dataset = TensorDataset(torch_features.float() , torch_target.float())\n",
        "\n",
        "# Return the last element of this dataset\n",
        "print(dataset[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZsgrFiLINvs",
        "outputId": "eefb7659-ed5c-4bea-e503-019fa4926eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.8883, 0.4074, 0.3171, 0.4329, 0.2112, 0.7095, 0.4987, 0.1853]), tensor([0.2213]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Extract the features (ph, Sulfate, Conductivity, Organic_carbon) and target (Potability) values and load them into tensors to represent features and targets.\n",
        "Use both tensors to generate a PyTorch dataset using the tensor dataset class.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Load the different columns into two PyTorch tensors\n",
        "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
        "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
        "\n",
        "# Create a dataset from the two generated tensors\n",
        "dataset = TensorDataset(features, target)"
      ],
      "metadata": {
        "id": "LqYNeFg2LV80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating model performance"
      ],
      "metadata": {
        "id": "L-guCsXeMU_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating training loss**"
      ],
      "metadata": {
        "id": "kc8f6gT4Tc0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = 0.0\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    # Run the forward pass\n",
        "    ...\n",
        "    # Calculate the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "    # Calculate the gradients\n",
        "    ...\n",
        "    # Calculate and sum the loss\n",
        "    training_loss += loss.item()  #### The loss tensor's .item() method returns the Python number contained in the tensor\n",
        "epoch_loss = training_loss / len(trainloader)"
      ],
      "metadata": {
        "id": "0B8zobI2MWc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating validation loss**"
      ],
      "metadata": {
        "id": "Rlof0xGtTt4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_loss = 0.0\n",
        "model.eval() # Put model in evaluation mode because some layers in PyTorch models behave differently at training versus validation stages\n",
        "with torch.no_grad(): # Speed up the forward pass\n",
        "    for i, data in enumerate(validationloader, 0):\n",
        "        # Run the forward pass\n",
        "        ...\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        validation_loss += loss.item()\n",
        "epoch_loss = validation_loss / len(validationloader)\n",
        "model.train() ### We set the model back to training mode at the end of the validation epoch, so we can run another training epoch."
      ],
      "metadata": {
        "id": "k8cNKm0yTvm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating accuracy with torchmetric**"
      ],
      "metadata": {
        "id": "LuuRqz_rVmns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In addition to loss, we also want to keep track of other metrics to evaluate how well our model is at predicting correct answers.\n",
        "To do so, a new package called torchmetrics will be used. If we are performing classification, we can use torchmetrics to create an accuracy metric.\n",
        "\n",
        "On each iteration of dataloader, we call this metric using model outputs and ground truth labels.\n",
        "The accuracy metric takes probabilities and single number labels as inputs. The output variable here would be the probabilities returned by the softmax function.\n",
        "If the labels contain one-hot encoded classes, we'll need the argmax function to obtain numbers instead of one-hot vectors.\n",
        "\n",
        "At the end of the epoch, we calculate total accuracy using the metric's .compute() method. Finally, we use .reset() to reset the metric for the next epoch.\n",
        "Accuracy is calculated in the same way for training and validation.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torchmetrics\n",
        "# Create accuracy metric using torch metrics\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
        "for i, data in enumerate(dataloader, 0):\n",
        "    features, labels = data\n",
        "    outputs = model(features)\n",
        "    # Calculate accuracy over the batch\n",
        "    acc = metric(outputs, labels.argmax(dim=-1))\n",
        "# Calculate accuracy over the whole epoch\n",
        "acc = metric.compute()\n",
        "print(f\"Accuracy on all data: {acc}\")\n",
        "# Reset the metric for the next epoch (training or validation)\n",
        "metric.reset()"
      ],
      "metadata": {
        "id": "rh31GWdHVpJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In this exercise, you will practice writing the evaluation loop. Recall that the evaluation loop is similar to the training loop, except that you will not perform the gradient calculation and the optimizer step.\n",
        "\n",
        "The model has already been defined for you, along with the object validationloader, which is a dataset.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Set the model to evaluation mode.\n",
        "Sum the current batch loss to the validation_loss variable\n",
        "Calculate the mean loss value for the epoch.\n",
        "Set the model back to training mode.\n",
        "\"\"\"\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "validation_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  for data in validationloader:\n",
        "\n",
        "      outputs = model(data[0])\n",
        "      loss = criterion(outputs, data[1])\n",
        "\n",
        "      # Sum the current loss to the validation_loss variable\n",
        "      validation_loss += loss.item()\n",
        "\n",
        "# Calculate the mean loss value\n",
        "validation_loss_epoch = validation_loss / len(validationloader)\n",
        "print(validation_loss_epoch)\n",
        "\n",
        "# Set the model back to training mode\n",
        "model.train()\n"
      ],
      "metadata": {
        "id": "92XDp9NcWZ4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In this exercise, you will practice using the torchmetrics package to calculate the accuracy. You will be using a sample of the facemask dataset.\n",
        "This dataset contains three different classes. The plot_errors function will display samples where the model predictions do not match the ground truth.\n",
        "\n",
        "The torchmetrics package is already imported. The model outputs are the probabilities returned by a softmax as the last step of the model. T\n",
        "he labels tensor contains the labels as one-hot encoded vectors\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Create an accuracy metric for a \"multiclass\" problem with three classes.\n",
        "Calculate the accuracy for each batch of the dataloader\n",
        "\n",
        "Calculate accuracy for the epoch.\n",
        "Reset the metric for the next epoch.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Create accuracy metric using torch metrics\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
        "for data in dataloader:\n",
        "    features, labels = data\n",
        "    outputs = model(features)\n",
        "\n",
        "    # Calculate accuracy over the batch\n",
        "    acc = metric(outputs, labels.argmax(dim=-1))\n",
        "\n",
        "# Calculate accuracy over the whole epoch\n",
        "acc = metric.compute()\n",
        "\n",
        "# Reset the metric for the next epoch\n",
        "metric.reset()\n",
        "plot_errors(model, dataloader)"
      ],
      "metadata": {
        "id": "tvjg-5DaWuuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fighting overfitting"
      ],
      "metadata": {
        "id": "T4mLyFowZIxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "To counter overfitting, we can reduce the model size or add a new type of layer called dropout. We can also use weight decay to force the parameters to remain small.\n",
        "We can get more data or use data augmentation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ScECq-G5ZK06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization\" using a dropout layer**"
      ],
      "metadata": {
        "id": "UdszcCDdbVsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Dropout is a \"regularization\" technique where randomly, a fraction of input neurons is set to zero at each update, effectively \"dropping\" them out.\n",
        "Corresponding connections are temporarily removed from the network, making the network less likely to overly rely on specific features.\n",
        "\n",
        "Dropout can be added to models as shown. The p argument indicates the probability of setting a neuron to zero.\n",
        "Here, we set 50 percent of the output tensor's neurons to zero. Usually, dropout layers are added after activation functions.\n",
        "Dropout layers behave differently between training and evaluation and we must not forget to switch the model mode\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5)\n",
        "  )\n",
        "features = torch.randn((1, 8))\n",
        "model(i)"
      ],
      "metadata": {
        "id": "1HEN41HpbXpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization with weight decay**"
      ],
      "metadata": {
        "id": "3ggN9b76b4h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "In PyTorch, weight decay can be added to the optimizer as shown. It is controlled by the weight_decay parameter,\n",
        "which should range between zero and one but is typically very small. When the optimizer's weight_decay parameter is set,\n",
        "it adds an additional term to the parameter update step that encourages smaller weights.\n",
        "\n",
        "This regularization term is proportional to the current value of the weight, and it is subtracted from the gradient during backpropagation.\n",
        "The weight decay term effectively penalizes large weights and helps prevent overfitting.\n",
        "The higher we set this parameter, the less likely our model is to overfit, so the model can generalize better to new data.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "FqnM-jrKboAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**"
      ],
      "metadata": {
        "id": "lyTaSQ9NcXWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Getting more data can be costly. However, researchers have found a way to artificially increase the size and diversity of their dataset by using data augmentation.\n",
        "Data augmentation is commonly applied to image data, which can be rotated and scaled, so that different views of the same face become available as \"new\" data points.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "voAeH-SWcbr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving model performance"
      ],
      "metadata": {
        "id": "1I-P6kKVdqFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Randomly sample a learning rate factor between 2 and 4 so that the learning rate (lr) is bounded between\n",
        "10e-2 and 10e-4\n",
        "\n",
        "Randomly sample a momentum between 0.85 and 0.99.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "values = []\n",
        "for idx in range(10):\n",
        "    # Randomly sample a learning rate factor between 2 and 4\n",
        "    factor = np.random.uniform(2, 4)\n",
        "    lr = 10 ** -factor\n",
        "\n",
        "    # Randomly select a momentum between 0.85 and 0.99\n",
        "    momentum = np.random.uniform(.85 , .99)\n",
        "\n",
        "    values.append((lr, momentum))"
      ],
      "metadata": {
        "id": "k9HS9U6vdudz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}