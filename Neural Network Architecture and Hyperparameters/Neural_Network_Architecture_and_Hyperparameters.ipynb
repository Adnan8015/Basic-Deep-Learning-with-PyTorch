{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIRoCFCC3_3d"
      },
      "source": [
        "# Discovering activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glXDrAFD4EXO"
      },
      "source": [
        "**Limitations of the Sigmoid and Softmax function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHpOZYLb2L49"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "The outputs of the sigmoid are bounded between zero and one, meaning that for any input, the output will be greater than zero and less than one.\n",
        "Unlike the softmax function, it can be used anywhere in the network.\n",
        "\n",
        "When we plot out the derivatives (or gradients) of the sigmoid function, we find that the gradients are always low and approach zero for low and high values of x.\n",
        "This behavior is called  \"\"SATURATION\"\". This property of the sigmoid function creates a challenge during backpropagation\n",
        "because each local gradient is a function of the previous gradient. For high and low values of x, the gradient will be so small\n",
        "that it can prevent the weight from changing or updating.\n",
        "\n",
        "This phenomenon is called vanishing gradients and makes training the network challenging.\n",
        "Because each element of the output vector of a softmax activation function is also bounded\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlY-nXI65J4v"
      },
      "source": [
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB9VArqj5MLp"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "The ReLU function outputs the maximum between its input and zero. For positive inputs, the output of the function is equal to the input.\n",
        "For strictly negative outputs, the output of the function is equal to zero.\n",
        "\n",
        "This function does not have an upper bound and the gradients do not converge to zero for high values of x, which overcomes the vanishing gradients problem.\n",
        "In PyTorch, the ReLU function may be called using the nn module.\n",
        "\n",
        "\n",
        "relu = nn.ReLU()\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vgMeWdP5gTw"
      },
      "source": [
        "**Leaky ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2Eyl7nr5icW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "The leaky ReLU is a variation of the ReLU function. For positive inputs, it behaves similarly to the ReLU function.\n",
        "For negative inputs, however, it multiplies them by a small coefficient (defaulted to zero.zero-one in PyTorch).\n",
        "By doing this, the leaky ReLU function has non-null gradients for negative inputs.\n",
        "\n",
        "In PyTorch, the leaky ReLU function is called using the nn module as well. The negative_slope parameter indicates the coefficient by which\n",
        "negative inputs are multiplied.\n",
        "\n",
        "leaky_relu = nn.LeakyReLU(negative_slope = 0.05)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgdxchht6ATB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass.\n",
        "Find the gradient at x\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a ReLU function with PyTorch\n",
        "relu_pytorch = nn.ReLU()\n",
        "\n",
        "# Apply your ReLU function on x, and calculate gradients\n",
        "x = torch.tensor(-1.0, requires_grad=True)\n",
        "y = relu_pytorch(x)\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient of the ReLU function for x\n",
        "gradient = x.grad\n",
        "print(gradient)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDgjYnzq9B1U"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
        "Call the function on the tensor x, which has already been defined for you.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a leaky relu function in PyTorch\n",
        "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
        "\n",
        "x = torch.tensor(-2.0)\n",
        "# Call the above function on the tensor x\n",
        "output = leaky_relu_pytorch(x)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB_tcS7-9LZT"
      },
      "source": [
        "# A deeper dive into neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XYsiVFS94HW"
      },
      "source": [
        "**Counting the number of parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkYs9rW79Pft"
      },
      "outputs": [],
      "source": [
        "### The capacity of a network reflects the number of parameters in said network.\n",
        "\n",
        "total = 0\n",
        "for parameter in model.parameters():\n",
        "  total += parameter.numel()\n",
        "print(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TclhgYqC-Q-J"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create a 3-layer linear neural network with <120 parameters, using n_features as input and n_classes as output sizes\n",
        "\"\"\"\n",
        "\n",
        "def calculate_capacity(model):\n",
        "  total = 0\n",
        "  for p in model.parameters():\n",
        "    total += p.numel()\n",
        "  return total\n",
        "\n",
        "n_features = 8\n",
        "n_classes = 2\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Create a neural network with less than 120 parameters\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(n_features , 4),\n",
        "    nn.Linear(4 , 3),\n",
        "    nn.Linear(3 , n_classes)\n",
        ")\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(calculate_capacity(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLrdGBHP-iRy"
      },
      "source": [
        "# Learning rate and momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwwGpQ3f3-Wv"
      },
      "source": [
        "**Momentum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejwHt4TQ-j_X"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "This parameter provides momentum to the optimizer enabling it to overcome local dips. The momentum keeps the step size large when previous steps were also large,\n",
        "even if the current gradient is small\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkJJ0xaA4SYL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "two parameters of an optimizer can be adjusted when training a model: the learning rate and the momentum.\n",
        "The learning rate controls the step size taken by the optimizer. Typical learning rate values range from ten raised to minus two, to ten raised to minus four.\n",
        "If the learning rate is too high, the optimizer may never be able to minimize the loss function. If it is too low, training may take longer.\n",
        "\n",
        "\n",
        "The other parameter, momentum, controls the inertia of the optimizer. Without momentum, the optimizer may get stuck in a local optimum.\n",
        "Momentum usually ranges from 0.85 to 0.99\n",
        "\n",
        "\n",
        "sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XX2a6dx4yW4"
      },
      "source": [
        "# Layers initialization,transfer learning and fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzYYgNcN6wB7"
      },
      "source": [
        "**Layer Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNXGxE2869lB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "In general, when training a neural network, we initialize the weights of each layer to small values. So why is this required? We previously discuss how the output of a neuron in a linear layer is a weighted sum of the outputs of the previous layer. By keeping both the input data and the layer's weights small we ensure that the outputs of our layers remain small. A layer can be initialized in different ways, for example using the uniform distribution. Layer initialization is an active topic of research.\n",
        "\n",
        "\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwyHR-oc41mG",
        "outputId": "7b0c2d18-1911-4111-8b0e-ec610850f967"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(-0.1249, grad_fn=<MinBackward1>) tensor(0.1250, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "layer = nn.Linear(64, 128)\n",
        "print(layer.weight.min(), layer.weight.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRx9wfXEDuPY"
      },
      "source": [
        "**Transfer learning and fine tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEKPCMKhDeVk"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "In practice, machine learning engineer are rarely training a model from randomly initialized weights.\n",
        "Instead they rely on a very powerful concept called transfer learning. Transfer learning consists in taking a model that was trained on a first task\n",
        "and reuse for a second task.\n",
        "\n",
        "For example, we trained a model on a large dataset of data scientist in the US and some more data became available, this time of salaries in Europe.\n",
        "Instead of training a model using randomly initialized weights, we can load the weights from the first model and use them as a starting point to train on this new dataset.\n",
        "Saving and loading weights in pytorch can be done using the torch.save and the torch.load functions.\n",
        "These functions works on any type of pytorch objects, whether it's a single layer or a full model.\n",
        "\n",
        "\n",
        "import torch\n",
        "layer = nn.Linear(64, 128)\n",
        "torch.save(layer, 'layer.pth')\n",
        "new_layer = torch.load('layer.pth')\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJsFyuOzEVQr"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Sometimes, the second task is similar to the first task and we want to perform a specific type of transfer learning, called fine-tuning.\n",
        "In this case, we load weights from a previously trained model, but train the model with a smaller learning rate. We can even train part of a network,\n",
        "if we decide some of the network layers do not need to be trained and choose to freeze them.\n",
        "\n",
        "A rule of thumb is to freeze early layers of the network and fine-tune layers closer to the output layer.\n",
        "This can be achieved by setting each parameter's requires_grad attribute to False. Here, we use the model's named_parameters() method,\n",
        "which returns the name and the parameter itself. We set requires_grad of the first layer's weight to False.\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(64, 128),\n",
        "      nn.Linear(128, 256)\n",
        "  )\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if name == '0.weight':\n",
        "    param.requires_grad = False\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVLvYJdHE4Ps"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers.\n",
        "However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.\n",
        "\n",
        "The model has already been created and exists under the variable model. You will be using the named_parameters method of the model to list the parameters of the model.\n",
        "Each parameter is described by a name. This name is a string with the following naming convention: x.name where x is the index of the layer.\n",
        "\n",
        "Remember that a linear layer has two parameters: the weight and the bias.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Use an if statement to determine if the parameter should be frozen or not based on its name.\n",
        "Freeze the parameters of the first two layers of this model.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "\n",
        "    # Check if the parameters belong to the first layer\n",
        "    if name == '0.weight' or name == '0.bias':\n",
        "\n",
        "        # Freeze the parameters\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Check if the parameters belong to the second layer\n",
        "    if name == '1.weight' or name == '1.bias':\n",
        "\n",
        "        # Freeze the parameters\n",
        "        param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHr9suafFl-x"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "For each layer (layer0 and layer1), use the uniform initialization method to initialize the weights.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "layer0 = nn.Linear(16, 32)\n",
        "layer1 = nn.Linear(32, 64)\n",
        "\n",
        "# Use uniform initialization for layer0 and layer1 weights\n",
        "nn.init.uniform_(layer0.weight)\n",
        "nn.init.uniform_(layer1.weight)\n",
        "\n",
        "model = nn.Sequential(layer0, layer1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
