{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Running Forward Pass**"
      ],
      "metadata": {
        "id": "m5Vjq2rfWWKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehO356BiUe4A"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generating predictions from models is called \"running a forward pass\" through a network.\n",
        "\n",
        "The purpose of the forward pass is to propagate input data through the network and produce predictions or outputs based on the model's learned parameters\n",
        "(weights and biases)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Binary Classification : forward pass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        " # Create input data of shape 5x6\n",
        "input_data = torch.tensor(\n",
        "          [[-0.4421,  1.5207,  2.0607, -0.3647,  0.4691,  0.0946],\n",
        "          [-0.9155, -0.0475, -1.3645,  0.6336, -1.9520, -0.3398],\n",
        "          [ 0.7406,  1.6763, -0.8511,  0.2432,  0.1123, -0.0633],\n",
        "          [-1.6630, -0.0718, -0.1285,  0.5396, -0.0288, -0.8622],\n",
        "          [-0.7413,  1.7920, -0.0883, -0.6685,  0.4745, -0.4245]]\n",
        ")\n",
        "\n",
        "# Create binary classification model\n",
        "model = nn.Sequential(\n",
        "      nn.Linear(6, 4),\n",
        "      nn.Linear(4, 1),\n",
        "      nn.Sigmoid() # Sigmoid activation function\n",
        ")\n",
        "# Pass input data through model\n",
        "output = model(input_data)\n",
        "print(output) ### This output will not be meaningful until we use backpropagation to update layer weights and biases."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mUQPmsCWez1",
        "outputId": "5539eb84-00f7-40da-8355-d94e3a1da083"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5536],\n",
            "        [0.6728],\n",
            "        [0.6307],\n",
            "        [0.5683],\n",
            "        [0.6266]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Multi-class classification: forward pass\n",
        "\n",
        "\n",
        "\n",
        "# Specify model has three classes\n",
        "n_classes = 3\n",
        "# Create multiclass classification model\n",
        "model = nn.Sequential(\n",
        "      nn.Linear(6, 4),\n",
        "      nn.Linear(4, n_classes),\n",
        "      nn.Softmax(dim=-1) # Softmax activation\n",
        ")\n",
        "# Pass input data through model\n",
        "output = model(input_data)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IKyEi40Xmye",
        "outputId": "d2339a96-95fa-4731-8546-34dd7b4366eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[0.2810, 0.5063, 0.2127],\n",
            "        [0.3713, 0.3129, 0.3158],\n",
            "        [0.2479, 0.3024, 0.4497],\n",
            "        [0.3295, 0.4224, 0.2481],\n",
            "        [0.2228, 0.3656, 0.4116]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Regression: forward pass\n",
        "\n",
        "# Create regression model\n",
        "model = nn.Sequential(\n",
        "      nn.Linear(6, 4), # First linear layer\n",
        "      nn.Linear(4, 1) # Second linear layer\n",
        ")\n",
        "# Pass input data through model\n",
        "output = model(input_data)\n",
        "# Return output\n",
        "print(output)"
      ],
      "metadata": {
        "id": "QMtK1L5rYV3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create a neural network that takes a tensor of dimensions 1x8 as input, and returns an output of the correct shape for binary classification.\n",
        "Pass the output of the linear layer to a sigmoid, which both takes in and return a single float.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(8 , 1),\n",
        "    nn.sigmoid()\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "TOCO5RotYgmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a 4-layer linear neural network compatible with input_tensor as the input, and a regression value as output.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Implement a neural network with exactly four linear layers\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(11 , 10),\n",
        "    nn.Linear(10 , 8),\n",
        "    nn.Linear(8 , 5),\n",
        "    nn.Linear(5 , 1),\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "CmBujnT-ZODi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Update the network provided to perform a multi-class classification with four outputs.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Update network below to perform a multi-class classification with four labels\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(11, 20),\n",
        "  nn.Linear(20, 12),\n",
        "  nn.Linear(12, 6),\n",
        "  nn.Linear(6, 4),\n",
        "  nn.Softmax(dim=-1)\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "isfWeGIKZZSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using loss functions\n",
        " to assess model\n",
        " predictions**"
      ],
      "metadata": {
        "id": "gOlwHlqKZmt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "The loss function tells us how good our model is at making predictions during training. It takes a model prediction, y-hat, and true label, or ground truth, y,\n",
        "as inputs, and outputs a float.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Oy0rv1wBZowY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross entropy loss in PyTorch\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "scores = torch.tensor([[-0.1211,  0.1059]])\n",
        "one_hot_target = torch.tensor([[1, 0]])\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "criterion(scores.double(), one_hot_target.double())  ### The output shown is the loss value."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Pw_8cWjd9y",
        "outputId": "5df0f60c-1c68-438f-8c20-8a050d1c963d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.8131, dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Creating one-hot encoded labels\n",
        "\n",
        "\"\"\"\n",
        "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset.\n",
        "This vector only contains zeros and ones. In this exercise, you'll create the one-hot encoded vector of the label y provided.\n",
        "\n",
        "You'll practice doing this manually, and then make your life easier by leveraging the help of PyTorch! Your dataset contains three classes,\n",
        "and the class labels range from 0 to 2 (e.g., 0, 1, 2).\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n",
        "Create a one-hot encoded vector of the ground truth label y using PyTorch.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import torch.nn.functional as f\n",
        "import numpy as np\n",
        "y = 1\n",
        "num_classes = 3\n",
        "\n",
        "# Create the one-hot encoded vector using NumPy\n",
        "one_hot_numpy = np.array([0, 1, 0])\n",
        "\n",
        "# Create the one-hot encoded vector using PyTorch\n",
        "one_hot_pytorch =f.one_hot(torch.tensor(1) , num_classes)"
      ],
      "metadata": {
        "id": "6R300yULkTW1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create the one-hot encoded vector of the ground truth label y, with 4 features (one for each class), and assign it to one_hot_label.\n",
        "\n",
        "Create the cross entropy loss function and store it as criterion.\n",
        "\n",
        "Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "y = [2]\n",
        "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
        "\n",
        "# Create a one-hot encoded vector of the label y\n",
        "one_hot_label = F.one_hot(torch.tensor(y), num_classes=4)\n",
        "\n",
        "# Create the cross entropy loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Calculate the cross entropy loss\n",
        "loss = criterion(scores.double() , one_hot_label.double())\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXg2XStClRKs",
        "outputId": "1dade93e-d530-4a24-c409-6f11bc1ab7ca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.0619, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using derivatives to\n",
        " update model\n",
        " parameters**"
      ],
      "metadata": {
        "id": "ooBRitF3mUVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### An analogy for derivatives\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Let's think of the loss function we are trying to minimize as a valley. Each horizontal step (along x) involves gaining or losing some height (y).\n",
        "At steeper slopes, a single step means losing or gaining a lot of elevation. Mathematically, the derivative (or slope) is high.\n",
        "Inversely, at gentler slopes, a single step involves losing or gaining less elevation, meaning a smaller derivative, or slope.\n",
        "Finally, the bottom of the valley is flat, and elevation does not change at each step: the derivative is null.\n",
        "\n",
        "If the valley is our loss function, the function is at minimum when the derivative is null.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RLRyMRUdmVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Backpropagation in PyTorch\n",
        "\n",
        "\n",
        "# Create the model and run a forward pass\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(16, 8),\n",
        "    nn.Linear(8, 4),\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "prediction = model(sample)\n",
        "# Calculate the loss and compute the gradients\n",
        "criterion = CrossEntropyLoss()\n",
        "loss = criterion(prediction, target)\n",
        "loss.backward()\n",
        "\n",
        "# Access each layer's gradients\n",
        "model[0].weight.grad, model[0].bias.grad\n",
        "model[1].weight.grad, model[1].bias.grad\n",
        "model[2].weight.grad, model[2].bias.grad"
      ],
      "metadata": {
        "id": "Abb23CYKmzK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Updating model parameters\n",
        "\n",
        "# Learning rate is typically small\n",
        "lr = 0.001\n",
        "\n",
        "# Update the weights\n",
        "weight = model[0].weight\n",
        "weight_grad = model[0].weight.grad\n",
        "weight = weight - lr * weight_grad\n",
        "\n",
        "# Update the biases\n",
        "bias = model[0].bias\n",
        "bias_grad = model[0].bias.grad\n",
        "bias = bias - lr * bias_grad"
      ],
      "metadata": {
        "id": "MILyxrUpvJ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Gradient Descent\n",
        "\n",
        "\"\"\"\n",
        "loss functions used in deep learning are not convex! To find global minima of non-convex functions, we use a mechanism called \"gradient descent\".\n",
        "PyTorch does this for us using \"optimizers\".\n",
        "\n",
        "The most common optimizer is stochastic gradient descent (SGD). We use optim to instantiate SGD as shown.\n",
        ".parameters() returns an iterable of all model parameters, which we pass to the optimizer. We use a standard learning rate, \"lr\", here, but this is tunable.\n",
        "The optimizer calculates gradients for us, and updates model parameters automatically, by calling .step().\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch.optim as optim\n",
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "wjPhvoy-vdfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Access the weight parameter of the first linear layer.\n",
        "Access the bias parameter of the second linear layer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "model = nn.Sequential(nn.Linear(16, 8),\n",
        "                      nn.Linear(8, 2)\n",
        "                     )\n",
        "\n",
        "# Access the weight of the first linear layer\n",
        "weight_0 = model[0].weight\n",
        "\n",
        "# Access the bias of the second linear layer\n",
        "bias_1 = model[1].bias"
      ],
      "metadata": {
        "id": "bjB5oNo4v30s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Create the gradient variables by accessing the local gradients of each weight tensor.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "weight0 = model[0].weight\n",
        "weight1 = model[1].weight\n",
        "weight2 = model[2].weight\n",
        "\n",
        "# Access the gradients of the weight of each linear layer\n",
        "grads0 = weight0.grad\n",
        "grads1 = weight1.grad\n",
        "grads2 = weight2.grad"
      ],
      "metadata": {
        "id": "5NlGYgOlwTc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Update the weights using the gradients scaled by the learning rate.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "weight0 = model[0].weight\n",
        "weight1 = model[1].weight\n",
        "weight2 = model[2].weight\n",
        "\n",
        "# Access the gradients of the weight of each linear layer\n",
        "grads0 = weight0.grad\n",
        "grads1 = weight1.grad\n",
        "grads2 = weight2.grad\n",
        "\n",
        "# Update the weights using the learning rate and the gradients\n",
        "weight0 = weight0 - lr * grads0\n",
        "weight1 = weight1 - lr * grads1\n",
        "weight2 = weight2 - lr * grads2"
      ],
      "metadata": {
        "id": "u7uWoqe8wZNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided.\n",
        "Update the model's parameters using the optimizer.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create the optimizer\n",
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(model.parameters() , lr = 0.001)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "loss = criterion(pred, target)\n",
        "loss.backward()\n",
        "\n",
        "# Update the model's parameters using the optimizer\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "rSi4BD-QxCHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "8EwETeGlx6eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Before the training loop\n",
        "\n",
        "\n",
        "# Create the dataset and the dataloader\n",
        "dataset = TensorDataset(torch.tensor(features).float(), torch.tensor(target).float())\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)  ### DataLoader() enables us to create \"batches\" of data, that are passed through the model in each forward/backward pass\n",
        "\n",
        "# Create the model\n",
        "model = nn.Sequential(nn.Linear(4, 2),\n",
        "nn.Linear(2, 1))\n",
        "\n",
        "# Create the loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "QOO-qtVKx-Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The training loop\n",
        " # Loop through the dataset multiple times\n",
        "for epoch in range(num_epochs):\n",
        "    for data in dataloader:\n",
        "        # Set the gradients to zero, bcz optimizer stores gradients from previous steps by default\n",
        "        optimizer.zero_grad()\n",
        "        # Get feature and target from the data loader\n",
        "        feature, target = data\n",
        "        # Run a forward pass\n",
        "        pred = model(feature)\n",
        "        # Compute loss and gradients\n",
        "        loss = criterion(pred, target)\n",
        "        loss.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "rcDU2LVO04xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Calculate the MSE loss using NumPy.\n",
        "Create a MSE loss function using PyTorch.\n",
        "Convert y_pred and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "y_pred = np.array(10)\n",
        "y = np.array(1)\n",
        "\n",
        "# Calculate the MSELoss using NumPy\n",
        "mse_numpy = np.mean((y_pred - y)**2)\n",
        "\n",
        "# Create the MSELoss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Calculate the MSELoss using the created loss function\n",
        "# mse_pytorch = criterion(y, y_pred)\n",
        "mse_pytorch = criterion(torch.tensor(y_pred).float(), torch.tensor(y).float())\n",
        "print(mse_pytorch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSOOvsHY1LQ-",
        "outputId": "75a71abe-45fa-464d-8de5-afbfb8574ad1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(81.)\n"
          ]
        }
      ]
    }
  ]
}